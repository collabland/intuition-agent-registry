{
  "https://node1.gaia.domains": {
    "address": "0x2a03ec4c563ac4567cfc27c54ad1e6efdd624331",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8084",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://qwen72b.gaia.domains": {
    "address": "0x57fa64fb75d1b8c778063adcd81d99e525b6197d",
    "chat": "Qwen2.5-72B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-72B-Instruct-Q5_K_M",
    "description": "The default GaiaNet node.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml-tool",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to directly answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0xc5795abafb1096aaeac0219608e91f8f8ac641b5.gaia.domains": {
    "address": "0xc5795abafb1096aaeac0219608e91f8f8ac641b5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x787d111af65a485ce25195419057a1179c1a35c7.gaia.domains": {
    "address": "0x787d111af65a485ce25195419057a1179c1a35c7",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x604d5de0ce1a7739c6e7fc056442b1dcacfbbdc5.gaia.domains": {
    "address": "0x604d5de0ce1a7739c6e7fc056442b1dcacfbbdc5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x4c03b6d528296c70cccfa527f0d6d28f8fd6d749.gaia.domains": {
    "address": "0x4c03b6d528296c70cccfa527f0d6d28f8fd6d749",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xd8f98f855abeeec03219447d5e15b18f574ea545.gaia.domains": {
    "address": "0xd8f98f855abeeec03219447d5e15b18f574ea545",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x2f80d864a9d8c443d105b77f18f85fd1ba70a8c8.gaia.domains": {
    "address": "0x2f80d864a9d8c443d105b77f18f85fd1ba70a8c8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xa0aa7eef552a08846901363fb93c97b38b1aebd8.gaia.domains": {
    "address": "0xa0aa7eef552a08846901363fb93c97b38b1aebd8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb2052377421049aa17d93832bc651c5610e6e219.gaia.domains": {
    "address": "0xb2052377421049aa17d93832bc651c5610e6e219",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x8e718208926d02c93e9e88ae3126e5ee431d3f17.gaia.domains": {
    "address": "0x8e718208926d02c93e9e88ae3126e5ee431d3f17",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x64185bbe5ef11bcfd737ec0b8a75a026e9d1c521.gaia.domains": {
    "address": "0x64185bbe5ef11bcfd737ec0b8a75a026e9d1c521",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://mgpwn3.gaia.domains": {
    "address": "0xa8103acdf3dc800e4524cc28f5c7036e4bcaf2f8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x81e30bc74f9ef31448146840d4772d99a084a479.gaia.domains": {
    "address": "0x81e30bc74f9ef31448146840d4772d99a084a479",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x76273205a36669ed6b6ba7be2cc29b346ea5d3d8.gaia.domains": {
    "address": "0x76273205a36669ed6b6ba7be2cc29b346ea5d3d8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x084dff79021b991b3f9db79d244759d6ecceebc8.gaia.domains": {
    "address": "0x084dff79021b991b3f9db79d244759d6ecceebc8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x3c47cfcc3625985d0eddec01f4319df12be2ed98.gaia.domains": {
    "address": "0x3c47cfcc3625985d0eddec01f4319df12be2ed98",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x0ee6a0c70896afb617f6425a6fb4011a75f9aed7.gaia.domains": {
    "address": "0x0ee6a0c70896afb617f6425a6fb4011a75f9aed7",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x2a6b09fa37f6c33ae04a9c023f5ecab0aa7c2ff7.gaia.domains": {
    "address": "0x2a6b09fa37f6c33ae04a9c023f5ecab0aa7c2ff7",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb06de941fa155037ae4168ef34a99efe54ed50ac.gaia.domains": {
    "address": "0xb06de941fa155037ae4168ef34a99efe54ed50ac",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x3a9d3a4ff62eca27f621a0b9be73c4c09a618f76.gaia.domains": {
    "address": "0x3a9d3a4ff62eca27f621a0b9be73c4c09a618f76",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x4f1a4d228e7f5b33214a16b8daa04ec3bbbe939d.gaia.domains": {
    "address": "0x4f1a4d228e7f5b33214a16b8daa04ec3bbbe939d",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x3e75f8b0e8bf3e01104160e96b9fa47bb2907684.gaia.domains": {
    "address": "0x3e75f8b0e8bf3e01104160e96b9fa47bb2907684",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x8afd1e91d344359a7303757d9039e00b0df9c088.gaia.domains": {
    "address": "0x8afd1e91d344359a7303757d9039e00b0df9c088",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x6344b62a7244019a54c21c8b98fd0d1659a423a5.gaia.domains": {
    "address": "0x6344b62a7244019a54c21c8b98fd0d1659a423a5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://guess.gaia.domains": {
    "address": "0x1101b0551bb10ba84c952f59b2c7e1ff59ef756b",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://jaha.gaia.domains": {
    "address": "0x7921cd1a430cbc0bd585119e4283aa593181b156",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xa4bfa105eae0f6348d4429c2dae4af61924bf51f.gaia.domains": {
    "address": "0xa4bfa105eae0f6348d4429c2dae4af61924bf51f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x002bad3d68842eb7d5d579fd5f864565a66ed4b6.gaia.domains": {
    "address": "0x002bad3d68842eb7d5d579fd5f864565a66ed4b6",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x858135e580bb709682adf6b13c160868a031f3f6.gaia.domains": {
    "address": "0x858135e580bb709682adf6b13c160868a031f3f6",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://vortex.gaia.domains": {
    "address": "0xcb62768782d449075346161b1ae7d01fc199158b",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8111",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x03a98332f98585d9815d13293e3d274def554c65.gaia.domains": {
    "address": "0x03a98332f98585d9815d13293e3d274def554c65",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://nodateka.gaia.domains": {
    "address": "0x1b87bd6314f9200197b9420f17a7bd4f7c9fe899",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://0x1623d295da35e98f7ec0e85ce6454103b6c16e00.gaia.domains": {
    "address": "0x1623d295da35e98f7ec0e85ce6454103b6c16e00",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x549111ecfdbf14782678e57fde9dd58bbc0dc43f.gaia.domains": {
    "address": "0x549111ecfdbf14782678e57fde9dd58bbc0dc43f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://isi-test.gaia.domains": {
    "address": "0x0db920829e4e8b0083ad42cb247188676466a652",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://a9club.gaia.domains": {
    "address": "0x710c2b97f397f9629351b19fcaf8242e285b1b20",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x0c82e25e1f996fa3d227d23e83cef721ee42ff69.gaia.domains": {
    "address": "0x0c82e25e1f996fa3d227d23e83cef721ee42ff69",
    "chat": "Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "32768",
    "chat_name": "Llama-3.2-3B-Instruct-Q5_K_M",
    "description": "LLM",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.2",
    "rag_policy": "system-message",
    "rag_prompt": "You are an experienced Rust developer. You will be asked to write or review code using Rust knowledge and best practices in the context below. You will be working with source code files in a typical Cargo project. The cargo.toml file contains the dependencies for external libraries and crates. The src/ directory contains main.rs or lib.rs or other Rust source code files. The project may also have SQL, text, HTML, Javascript, CSS and Python files. The user will give you the specific code files you need to work on.\n\n# Context for Rust knowledge related to the user task\n\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
    "system_prompt": "You are an intelligent programmer. You are happy to help answer any questions that the user has (usually they will be about coding). Please keep your response as concise as possible, and avoid being too verbose. Do not lie or make up facts. If a user messages you in a foreign language, please respond in that language. Format your response in markdown."
  },
  "https://0x3ba72a2c827aa7a7b50366abedd6498404019da2.gaia.domains": {
    "address": "0x3ba72a2c827aa7a7b50366abedd6498404019da2",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xc4c589a72c2a937c993895028aa159516bfdca55.gaia.domains": {
    "address": "0xc4c589a72c2a937c993895028aa159516bfdca55",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://dolis.gaia.domains": {
    "address": "0x63ff0c4a3129c7bc0100989724703936f30f3d84",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://godfather.gaia.domains": {
    "address": "0x5f965101a27022964deaaa0c5d654e7f6a4e2245",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://notax.gaia.domains": {
    "address": "0x62d1f22a218bb1dc557f5e9003ba9ebc9cf8a9ae",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://frays.gaia.domains": {
    "address": "0x7a1be74b58a4284e3eb9490cda1d005328ec26ea",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://torde.gaia.domains": {
    "address": "0x8f39aeaba578617946b9f7a62a864c62369316be",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://khorne.gaia.domains": {
    "address": "0xdb4b4f40c8799d91ccfc9c1200a47b379756b954",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://luckystrike.gaia.domains": {
    "address": "0xf0dec837d92e5f17113f079c7153dbe903e05347",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://frant.gaia.domains": {
    "address": "0x521f0d26eb60f025646ab4d909638e079f120d2d",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xa85901fba1736ba122e764068571b9793875c313.gaia.domains": {
    "address": "0xa85901fba1736ba122e764068571b9793875c313",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://citex.gaia.domains": {
    "address": "0xaf74c479ea1234a0e1430abab556b18241d317fe",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://moschinoo.gaia.domains": {
    "address": "0xde2d0b39dcc74cf711c3a8bf0d10fb9715656290",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://reeply.gaia.domains": {
    "address": "0x731bf4e842412d59636dbe9cc2be93fc6b3d75d4",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://dogss.gaia.domains": {
    "address": "0x04ce3802d89831d2b5df92732728277b50a90b77",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://flyster.gaia.domains": {
    "address": "0x98eaaa47381ae197b8e65a5c0dd53a4ae030991e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://cherokey.gaia.domains": {
    "address": "0x1817af70e143c3f2d33ab9849998a64ed48bca46",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://twitter.gaia.domains": {
    "address": "0xde2165dac21ece9f72424d2a2f08e65bd2dfae61",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x48762335bbfb659576b5ad99bde4c2f3443c44b6.gaia.domains": {
    "address": "0x48762335bbfb659576b5ad99bde4c2f3443c44b6",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://primierre.gaia.domains": {
    "address": "0x04afaebb01321d8c0bd41a72842998e0c08e4bc1",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://ebagger.gaia.domains": {
    "address": "0x5530c2819c5268a980762122f693455543b1a27d",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://artem.gaia.domains": {
    "address": "0xf5c5c481f125ae0b6ab7a6597330151468e72909",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://tiskey.gaia.domains": {
    "address": "0x839b087c6b2a95f7c0637cedc13c5dffcb85dc12",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://blabla.gaia.domains": {
    "address": "0x4254a87210af9bda67d4747f73ad691e45f4ceef",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://comedy.gaia.domains": {
    "address": "0x8764cef925f6ac1e6e0fdf528f7c3678aa0f7f6c",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://riluality.gaia.domains": {
    "address": "0xfda96a08ee3a64be35c86b7cfb7611211017a883",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://claster.gaia.domains": {
    "address": "0x7b96b4228ac1431d508d7847e9e5a13848dde35c",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://jully.gaia.domains": {
    "address": "0x2a6379d18f518794af8216e26cd749933315fbd5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://shogal.gaia.domains": {
    "address": "0xe29ab01a37968b92fa0dd624fed358f728e6a416",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://greengo.gaia.domains": {
    "address": "0xa87432ccc3b1072b74762cbf718de1cfb8735cd7",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://backer.gaia.domains": {
    "address": "0x3bd3e0b109d4f4670350e309b42207f9eb21e166",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://lablanca.gaia.domains": {
    "address": "0x11194e1c652b19bf449f03cd307dd805cf4d9ceb",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://amalis.gaia.domains": {
    "address": "0xe9fd746c744a4697e11e5dbd61b1e5ee106cd577",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://usdc.gaia.domains": {
    "address": "0x2ddf50e7043322a468ae4e7b29df81ad4f96fd80",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://robocop.gaia.domains": {
    "address": "0x63f03c33acfbdb10f9af3a48d84465a13606fabd",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://bigdestiny.gaia.domains": {
    "address": "0xeba483c1412d7a671b7fe9ab37958afb9e67813b",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://bafamet.gaia.domains": {
    "address": "0x71357c7826d20024db953376c3832318f92ae0cb",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://yurii.gaia.domains": {
    "address": "0x0ca482eabcc3cd06632d8b0eb03a007d47952c65",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x8d19508992d6415bd0c8fa48e98c7fb360f433ea.gaia.domains": {
    "address": "0x8d19508992d6415bd0c8fa48e98c7fb360f433ea",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x76c8b791f99de3b695037ae039c940373d8d2cb7.gaia.domains": {
    "address": "0x76c8b791f99de3b695037ae039c940373d8d2cb7",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x44738fb82be15ab65119657c49d95ed028d75f41.gaia.domains": {
    "address": "0x44738fb82be15ab65119657c49d95ed028d75f41",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://fonda.gaia.domains": {
    "address": "0x035eac9fa2f81bda445011ddf60f97f8b672eb6d",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://metamask.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://zksync.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://belfort.gaia.domains": {
    "address": "0x0a00146c0393e253a799375f9712c9c9c160a1d4",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "8",
    "chat_ctx_size": "8192",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "1024",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "4096",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "3",
    "qdrant_score_threshold": "0.7",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://consensus.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://polygon.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://bitcoin.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://avalanche.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://scroll.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://optimism.gaia.domains": {
    "address": "0xb4be2f87bed01f6dd7826024cbc4a095eded4d58",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://chainlink.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://base.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://mantle.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://0xf5a9e054b93ca431c0cac4db4a479fe6e2c4b43f.gaia.domains": {
    "address": "0xf5a9e054b93ca431c0cac4db4a479fe6e2c4b43f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://yumchatai.gaia.domains": {
    "address": "0x50218e5b4333bde3d55da043352bb8d0662dc4f4",
    "chat": "Llama-3.2-3B-Instruct-uncensored-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "32768",
    "chat_name": "Llama-3.2-3B-Instruct-uncensored-Q5_K_M",
    "description": "LLM",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed",
    "llamaedge_port": "8084",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.2",
    "rag_policy": "system-message",
    "rag_prompt": "Answer based on the following context",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x362b818aad6e18d946cc33021cb05fe03aa80e88.gaia.domains": {
    "address": "0x362b818aad6e18d946cc33021cb05fe03aa80e88",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x2825e7cef8f734698f0dad4f5c261e3a20a20bc1.gaia.domains": {
    "address": "0x2825e7cef8f734698f0dad4f5c261e3a20a20bc1",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x2043e4b9149d5bb26d6700cb72b03358452ee5e9.gaia.domains": {
    "address": "0x2043e4b9149d5bb26d6700cb72b03358452ee5e9",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://tadea.gaia.domains": {
    "address": "0xee9e776cbe5072ba23a7ccff0e2454a9fe5606e5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://botinok.gaia.domains": {
    "address": "0x3128ceff30ec2cca2e5f3a13d3abb3d5a55b78ae",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb61e10be32c97b055e4295b281274e4b04317aa8.gaia.domains": {
    "address": "0xb61e10be32c97b055e4295b281274e4b04317aa8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xfc4ee0274c9ea28d4f4156796da352d9d613fac3.gaia.domains": {
    "address": "0xfc4ee0274c9ea28d4f4156796da352d9d613fac3",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://meran.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://shishka.gaia.domains": {
    "address": "0x0a4acfa0a8166122cb6cdaf3c3c5074aed68e69b",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x0e88a9c90c263dc0a1bc912486ab60b6530a8439.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://lumos.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://upset.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://saxon.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://risk.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://dtna.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://creep.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://scent.gaia.domains": {
    "address": "0x304dff7f29e1397563e62fd4045efd871e840223",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "4096",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B model and blockchain knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "768",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "768",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "768",
    "llamaedge_chat_port": "7070",
    "llamaedge_embedding_port": "8081",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question. \nRespond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. \nNever mention your knowledge base or say 'according to the context' or other similar language. Make your answer as short as possible that suitable for the length of a tweet. \nThe terms gaia, gaianetwork, gaianet and gaia network all refer to the same project called Gaia. \nIf you don't know the answer, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/metamask-all.snapshot",
    "system_prompt": "You are an expert on blockchain project documentation, capable of answering questions related to the following projects stored in the Qdrant database: Gaia, Polygon, Consensys, MetaMask, Bitcoin, zkSync, Avalanche, Optimism, Scroll, Chainlink, Base, Mantle. Please follow these guidelines:\n\n1. **Accuracy**: If relevant information is found in the documents, provide a direct and accurate answer.\n2. **Brevity**: Answers should be concise and to the point, without unnecessary words.\n3. **Terminology**: Keep the original terminology from the project documentation without rewriting, replacing, or simplifying it.\n4. **Project Distinction**: Ensure that information from different projects is not mixed up or confused.\n5. **No Fabrication**: If the relevant information is not available in the documents, explicitly inform the user that the answer is unknown or not provided in the documentsdo not fabricate an answer.\n\n6. **General Queries**: If the user's question does not specifically relate to any project or its documentation, avoid referencing the RAG database and respond based on general knowledge or simply ask for clarification on the user's question."
  },
  "https://giga.gaia.domains": {
    "address": "0x4a76f58c79f7bf1b44ac133429e21daf1c22160c",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xd31e1d03189628bd22a7e1674491581a20d6188e.gaia.domains": {
    "address": "0xd31e1d03189628bd22a7e1674491581a20d6188e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xe0d9ba9833bc5b50fa78715af1e7f349232afb77.gaia.domains": {
    "address": "0xe0d9ba9833bc5b50fa78715af1e7f349232afb77",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://gashwin.gaia.domains": {
    "address": "0xa9c9abd9919fb86194d7cb7b37c936116590b42a",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://llama.gaia.domains": {
    "address": "0x53e134590f66693ae4dac5561509cf46b4f8b248",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x1684d9bf1568d55befd4109591316e6419ba946e.gaia.domains": {
    "address": "0x1684d9bf1568d55befd4109591316e6419ba946e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x3af53610c71a06c0fade722752ba93eab3974620.gaia.domains": {
    "address": "0x3af53610c71a06c0fade722752ba93eab3974620",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xeb6312a474c4073a323dffbd45411a903de50077.gaia.domains": {
    "address": "0xeb6312a474c4073a323dffbd45411a903de50077",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x632c25df1b6c31eb2c49be29f6f4ba11c9bf7074.gaia.domains": {
    "address": "0x632c25df1b6c31eb2c49be29f6f4ba11c9bf7074",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x17f3366ab11d39141310036d7cbcb294b650581e.gaia.domains": {
    "address": "0x17f3366ab11d39141310036d7cbcb294b650581e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xd8ef141f6320839c080238c3ae2f19750e232e36.gaia.domains": {
    "address": "0xd8ef141f6320839c080238c3ae2f19750e232e36",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-3B-Instruct-GGUF/resolve/main/Qwen2.5-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-3B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x511f9f42e6f063254f02df08b4c1371d7a84c797.gaia.domains": {
    "address": "0x511f9f42e6f063254f02df08b4c1371d7a84c797",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://pengu.gaia.domains": {
    "address": "0x8cc28c18c7abb2668cb9fa5899aa68486b4bb102",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xcc3877e490b235d5cd54b60fec58530e0d482995.gaia.domains": {
    "address": "0xcc3877e490b235d5cd54b60fec58530e0d482995",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xf6c10f77391ac6262a1aa9d3195b49a20c07c6ac.gaia.domains": {
    "address": "0xf6c10f77391ac6262a1aa9d3195b49a20c07c6ac",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x87bb8fb19453a5aafd20696e38fc527657fbe44b.gaia.domains": {
    "address": "0x87bb8fb19453a5aafd20696e38fc527657fbe44b",
    "chat": "https://huggingface.co/gaianet/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-MXFP4_MOE.gguf",
    "chat_batch_size": "64",
    "chat_ctx_size": "8192",
    "chat_name": "gpt-oss-20b",
    "context_window": "1",
    "description": "gpt-oss-20b",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed-text-v1.5.f16",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "gpt-oss",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "last-user-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant."
  },
  "https://0xd9a12f96ee34f3a9531f856b2f247e4431eb5514.gaia.domains": {
    "address": "0xd9a12f96ee34f3a9531f856b2f247e4431eb5514",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://ruesandora.gaia.domains": {
    "address": "0xe1bbf0ba96f5302a23b712b837ffa3384e32f290",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x2a1bdb1d85887f841b4b47132d94c06fd5ca7275.gaia.domains": {
    "address": "0x2a1bdb1d85887f841b4b47132d94c06fd5ca7275",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://defai.gaia.domains": {
    "address": "0xeca8a8ac5f14d0a82af8918bfb59fe450580d43c",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9058",
    "llamaedge_embedding_port": "9059",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://zerdrop.gaia.domains": {
    "address": "0x013116e8c29b88675da112dbafdba88eed8dc127",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x92be24bf1099852989a547c9c374eb2ae24be126.gaia.domains": {
    "address": "0x92be24bf1099852989a547c9c374eb2ae24be126",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://coconode.gaia.domains": {
    "address": "0x828cbc88364783f854f32d15b85d01a28c0524c2",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_port": "9090",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://pornhub.gaia.domains": {
    "address": "0x8baefc67bfd4d26c960094c232d469617b880bbb",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8079",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xfc554565e649a0ac7b1332097bbdc53caa9c1692.gaia.domains": {
    "address": "0xfc554565e649a0ac7b1332097bbdc53caa9c1692",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://hyper.gaia.domains": {
    "address": "0x378b0f3cb86fbf84c8447cad1c5694e337515506",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "chat_name": "Llama-3.2-3B-Instruct-Q5_K_M",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xf7636b4c3f4d6ffe93d2241c279559f721ad3454.gaia.domains": {
    "address": "0xf7636b4c3f4d6ffe93d2241c279559f721ad3454",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x9043755ccdb0f05f34c75f4cb99b3c9bbb27fa90.gaia.domains": {
    "address": "0x9043755ccdb0f05f34c75f4cb99b3c9bbb27fa90",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x192cf30beda08923de984193f5791131c8953578.gaia.domains": {
    "address": "0x192cf30beda08923de984193f5791131c8953578",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xc85e999d3d272583d325a597db4ea8bc12cead83.gaia.domains": {
    "address": "0xc85e999d3d272583d325a597db4ea8bc12cead83",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://plastq.gaia.domains": {
    "address": "0x13ecd55ebffe8490f6ff9547fef1ed817c07a985",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://antonprofit.gaia.domains": {
    "address": "0x7b45c6cb8c0d467dd25b86c620e2f5edf0ab8481",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x5b212e969cbc5e793ae2ccf20528447b261c7ba5.gaia.domains": {
    "address": "0x5b212e969cbc5e793ae2ccf20528447b261c7ba5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://professorgwei.gaia.domains": {
    "address": "0xff898d2eaf6bfe97274a6177ce7322adefe5881b",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "8192",
    "chat_name": "llama-3.2-3b",
    "description": "The default GaiaNet node config with a Llama 3.2 3B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant."
  },
  "https://0x59ed4acbbb1523e526cd4af761fbf86c06e374f5.gaia.domains": {
    "address": "0x59ed4acbbb1523e526cd4af761fbf86c06e374f5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x71315db8274ad3b012e5f569180cac2b7ac1b63e.gaia.domains": {
    "address": "0x71315db8274ad3b012e5f569180cac2b7ac1b63e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://academy-nodes.gaia.domains": {
    "address": "0xe614edfde6f0c75fa40f55437c741b88881e7899",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x10505eaabe3040d6fd8be65dcb495501e9b37d63.gaia.domains": {
    "address": "0x10505eaabe3040d6fd8be65dcb495501e9b37d63",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://porta.gaia.domains": {
    "address": "0x8b57dda3a74b1e8417a701849ee0f7dc0e791fe3",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://babycheuk.gaia.domains": {
    "address": "0xc989993596a941c7635346a154c15124d05c93d1",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://huggingface.gaia.domains": {
    "address": "0x530e4fc53087c69b8816ca041686e9fe9ccaa88f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://traders.gaia.domains": {
    "address": "0x0e89e7794542c52b1c56a38cb2787ac24ebc76a8",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://pantheon.gaia.domains": {
    "address": "0x0cbd51542dc2ebce7a851e593ba687e032fd6fa0",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x19c4338b6eefb902aca13b733603394b55c7bb94.gaia.domains": {
    "address": "0x19c4338b6eefb902aca13b733603394b55c7bb94",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xc8a4008ea00f66146d118cd9124bbeea90326fb5.gaia.domains": {
    "address": "0xc8a4008ea00f66146d118cd9124bbeea90326fb5",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "chat_name": "Llama-3.2-3B-Instruct-Q5_K_M",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://hercules.gaia.domains": {
    "address": "0xb82e153d2e6a0845a7e9fcb22ac1676a59afa800",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xd8004a5bb7e5c7c5d0ccc38dacef66f59466fb69.gaia.domains": {
    "address": "0xd8004a5bb7e5c7c5d0ccc38dacef66f59466fb69",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xf11adcf1e045580a52917cece2187733fb9b2aea.gaia.domains": {
    "address": "0xf11adcf1e045580a52917cece2187733fb9b2aea",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xe96ad6c31d2c407e839c005b7a3eee3adf70dea5.gaia.domains": {
    "address": "0xe96ad6c31d2c407e839c005b7a3eee3adf70dea5",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://newyork.gaia.domains": {
    "address": "0x620e5bf565234a376602864746e4b6fdddeb11ae",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb8b34b9be4cc9479fef59122a10320b0f1f87cfc.gaia.domains": {
    "address": "0xb8b34b9be4cc9479fef59122a10320b0f1f87cfc",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x7d8aff6bce5bc3ca728a032ed5b08395e1b355f0.gaia.domains": {
    "address": "0x7d8aff6bce5bc3ca728a032ed5b08395e1b355f0",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://darknet.gaia.domains": {
    "address": "0xc18b5f189300533b849f0fed526314fbe1d16571",
    "chat": "https://huggingface.co/gaianet/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-MXFP4_MOE.gguf",
    "chat_batch_size": "64",
    "chat_ctx_size": "8192",
    "chat_name": "gpt-oss-20b",
    "context_window": "1",
    "description": "gpt-oss-20b",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed-text-v1.5.f16",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "gpt-oss",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "last-user-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant."
  },
  "https://0x9ddde22ab54e851a65714389e2f21d57fcab016f.gaia.domains": {
    "address": "0x9ddde22ab54e851a65714389e2f21d57fcab016f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://openai.gaia.domains": {
    "address": "0x82ff7b198a70039c2b961feab7baaf7a375f5456",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://nick.gaia.domains": {
    "address": "0xeb1a9ff72c49f786f34829717a5fc1e66324d5d8",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x3b43e8177ed55e070b2b1853225b1adef03036cc.gaia.domains": {
    "address": "0x3b43e8177ed55e070b2b1853225b1adef03036cc",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x837a75bdbf30504c100ee0c4e9d55a66ce428c9a.gaia.domains": {
    "address": "0x837a75bdbf30504c100ee0c4e9d55a66ce428c9a",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xcaa40ddf1a89f22b753e54ade3d480ae4b34a9c7.gaia.domains": {
    "address": "0xcaa40ddf1a89f22b753e54ade3d480ae4b34a9c7",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x4f213cb40a9e267d6e818d03b1d2b3ffcca50708.gaia.domains": {
    "address": "0x4f213cb40a9e267d6e818d03b1d2b3ffcca50708",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x8bd9f9cdd830ae03bb5a8ab1d87d514b72992e35.gaia.domains": {
    "address": "0x8bd9f9cdd830ae03bb5a8ab1d87d514b72992e35",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://paoner.gaia.domains": {
    "address": "0xfff9ef155b3d6f83447b401b2c2288a4245d2e89",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x6fe25da42f723f5f9bc479616c6da633d3a3cc05.gaia.domains": {
    "address": "0x6fe25da42f723f5f9bc479616c6da633d3a3cc05",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://sitnikoffa.gaia.domains": {
    "address": "0xf44f6683fd92729ce55c82b7545447703d035c98",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://b0ban.gaia.domains": {
    "address": "0x5ebb691b6692f7fa4cdeac525e212eda97fe50cd",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb2f9ef465b83947998cf1db5da9adaf28443ef1e.gaia.domains": {
    "address": "0xb2f9ef465b83947998cf1db5da9adaf28443ef1e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x0ac06ca5ec0fc774ec90a815a15883b950206ddb.gaia.domains": {
    "address": "0x0ac06ca5ec0fc774ec90a815a15883b950206ddb",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x09d9f601a63b132d8338f8a00852f9e0caac89e2.gaia.domains": {
    "address": "0x09d9f601a63b132d8338f8a00852f9e0caac89e2",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x33adc1cca7de7cb8d326993678e300620d086d76.gaia.domains": {
    "address": "0x33adc1cca7de7cb8d326993678e300620d086d76",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xee5209929bfbe87d77f1a81d8952c464f86ca81e.gaia.domains": {
    "address": "0xee5209929bfbe87d77f1a81d8952c464f86ca81e",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xfb3cde7de1534e75ae89f71977313b5dcb0f2f8f.gaia.domains": {
    "address": "0xfb3cde7de1534e75ae89f71977313b5dcb0f2f8f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://aptos.gaia.domains": {
    "address": "0x31a5192e3e04bea3d1dc2618114892ee78983732",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x7bf1d050eb7e067ce7afec53cfccff9b7d66e142.gaia.domains": {
    "address": "0x7bf1d050eb7e067ce7afec53cfccff9b7d66e142",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x99ce592a90e150ac43decb18cbbb9eda9b187538.gaia.domains": {
    "address": "0x99ce592a90e150ac43decb18cbbb9eda9b187538",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x437c4d118e437a728ad547035e5b985295dcfe45.gaia.domains": {
    "address": "0x437c4d118e437a728ad547035e5b985295dcfe45",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "18081",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://dashou.gaia.domains": {
    "address": "0xdc3346f663cf02ffd3fd34b6d093008668663b40",
    "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "8192",
    "chat_name": "llama-3.2-3b",
    "description": "The default GaiaNet node config with a Llama 3.2 3B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant."
  },
  "https://smetana.gaia.domains": {
    "address": "0xb57e2ab29eabe39253244be40abbc08ba791c1c3",
    "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "32768",
    "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://ihollow.gaia.domains": {
    "address": "0x9569ca7eb8da8f65586f5fc6323f10f1067666ae",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x82ffceb8abb9e3cba93c1c6c9a630176479fa16f.gaia.domains": {
    "address": "0x82ffceb8abb9e3cba93c1c6c9a630176479fa16f",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xed48c1a1708e5a6930668f8bb866f382deedb914.gaia.domains": {
    "address": "0xed48c1a1708e5a6930668f8bb866f382deedb914",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8101",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xb204fe910bdca2b94034b48b8b95f02198b0fe10.gaia.domains": {
    "address": "0xb204fe910bdca2b94034b48b8b95f02198b0fe10",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0xc403b533599ca159f728e82fd941a25a1ae2cb88.gaia.domains": {
    "address": "0xc403b533599ca159f728e82fd941a25a1ae2cb88",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x4b38f4f3ada27f31aad500b13cbcbcdfe45d32dc.gaia.domains": {
    "address": "0x4b38f4f3ada27f31aad500b13cbcbcdfe45d32dc",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x0ac9fbbeed997aba2bc7f3eb1e7013423b9f96f9.gaia.domains": {
    "address": "0x0ac9fbbeed997aba2bc7f3eb1e7013423b9f96f9",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x709e82f42940553f1260f86d34382658e558832e.gaia.domains": {
    "address": "0x709e82f42940553f1260f86d34382658e558832e",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0x0a4b946e992f81a432a06017962a8783efd9d35d.gaia.domains": {
    "address": "0x0a4b946e992f81a432a06017962a8783efd9d35d",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://0xe2b4b1d79cd59a92d41c0d6ade813fc18c54fa4c.gaia.domains": {
    "address": "0xe2b4b1d79cd59a92d41c0d6ade813fc18c54fa4c",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8088",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x2553db37b601564ed6908094720e68950bf31aad.gaia.domains": {
    "address": "0x2553db37b601564ed6908094720e68950bf31aad",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "16",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "16",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "512",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://swags.gaia.domains": {
    "address": "0xd43ae11fef962236fa136d879e2486d42c70176c",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "4096",
    "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "The default GaiaNet node config",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use information in the following context to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful assistant."
  },
  "https://anon1.gaia.domains": {
    "address": "0x4842b27cdbec7a9b70f3d81d229b5709ac863c10",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://0x853ea9b08f1106aa165c1542c3874f2e8d559636.gaia.domains": {
    "address": "0x853ea9b08f1106aa165c1542c3874f2e8d559636",
    "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "32768",
    "chat_name": "Qwen2.5-Coder-0.5B-Instruct-Q5_K_M",
    "context_window": "1",
    "description": "Qwen 2.5 Coder 0.5B",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "nomic-embed",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.2",
    "rag_policy": "system-message",
    "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding)."
  },
  "https://0xdb6823515a5162d639dd50d89360332cb878a0e2.gaia.domains": {
    "address": "0xdb6823515a5162d639dd50d89360332cb878a0e2",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://untomation.gaia.domains": {
    "address": "0x89449e146d6e79433a46459d5691da1ebe44a006",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model and a Paris tour guide knowledge base.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "14441",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately."
  },
  "https://0xfed0fdaada6df80eade128dc7b4afcdc45348f0c.gaia.domains": {
    "address": "0xfed0fdaada6df80eade128dc7b4afcdc45348f0c",
    "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "32",
    "chat_ctx_size": "131072",
    "description": "This GaiaNet node config with a Qwen2 0.5B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "chatml",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
  },
  "https://llama3b.gaia.domains": {
    "address": "0x021b5decef58b81ebd4c2c469e29752f10255538",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x53550a54857dca63be7a83f9a1f0c8e4e3437386.gaia.domains": {
    "address": "0x53550a54857dca63be7a83f9a1f0c8e4e3437386",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x9761c3e6f3b4730dfc37273856f1bdb7b481600a.gaia.domains": {
    "address": "0x9761c3e6f3b4730dfc37273856f1bdb7b481600a",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x5e80f24fba85d68d356a7b4d2e5b1f6d23ae7295.gaia.domains": {
    "address": "0x5e80f24fba85d68d356a7b4d2e5b1f6d23ae7295",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x6e8ed131adde44cd62b2aee71ad2da423b249240.gaia.domains": {
    "address": "0x6e8ed131adde44cd62b2aee71ad2da423b249240",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9088",
    "llamaedge_embedding_port": "9089",
    "llamaedge_port": "8085",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/balrajdvo/heurist/resolve/main/default-3584675309035317-2025-07-17-19-27-51.snapshot.tar.gz",
    "system_prompt": "You are the Heurist Navigator, an expert AI assistant specialized in the Heurist ecosystem - a full-stack infrastructure platform for the AI economy. Your role is to help users understand and navigate Heurist's three core platforms: Heurist Cloud, Heurist Chain, and Heurist Digital Assets. Core Knowledge Areas: Heurist Cloud - Decentralized AI compute and agent marketplace - 50+ supported LLMs, image & video generation models - Heurist Mesh: composable agent marketplace with 30+ specialized agents - MCP and A2A protocol compatibility - Censorship-resistant inference APIs. Heurist Chain - Purpose-built Layer 2 for AI agent interactions - ZK stack powered micropayments - Ethereum security with ZK proofs - Autonomous agent-to-agent communication - MCP standards and A2A Protocol compatibility. Heurist Digital Assets - Regulated AI startup tokenization platform - Onchain equity tokenization - Global capital raising capabilities - Transparent cap table management. Flagship Applications - Heurist Imagine: 15+ image/video generation models - Pondera: Advanced conversational AI - Heurist Deep Research: Multi-agent web analysis (100+ pages in minutes). User Personas You Serve: Developers: Guide them through API integration, agent composition, and technical implementation. Crypto/Web3 Community: Explain tokenization, DeFi integration, and blockchain benefits. AI Builders: Help with model selection, agent orchestration, and scaling solutions. Entrepreneurs: Clarify funding mechanisms, tokenization processes, and ecosystem opportunities. Response Guidelines: Be Practical: Provide actionable guidance with specific examples. Technical Depth: Match complexity to user's expertise level. Ecosystem Focus: Always connect answers to broader Heurist infrastructure benefits. Use Case Driven: Relate features to real-world applications. Web3 Native: Emphasize decentralization, censorship resistance, and blockchain advantages. Key Value Propositions to Highlight: Unified Infrastructure: Single platform for compute, communication, and capital. Decentralization: Censorship resistance and reduced single points of failure. Interoperability: Seamless agent-to-agent communication and payments. Cost Efficiency: Pay-per-use pricing and optimized resource allocation. Innovation Friendly: Support for cutting-edge AI applications and Web3 integration. Always provide specific, helpful information about how users can leverage Heurist's infrastructure to build, scale, and fund their AI applications. When discussing technical implementations, reference relevant APIs, protocols, or platform features. For business inquiries, explain tokenization benefits and funding opportunities clearly."
  },
  "https://0xcead997cab900c82b9c195a68539aca3757fbff6.gaia.domains": {
    "address": "0xcead997cab900c82b9c195a68539aca3757fbff6",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xc45d89cb57f112c1254b89102cb7539ad1073c93.gaia.domains": {
    "address": "0xc45d89cb57f112c1254b89102cb7539ad1073c93",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xf2a9ae323da06acdfbbf48868388ce7be19f985b.gaia.domains": {
    "address": "0xf2a9ae323da06acdfbbf48868388ce7be19f985b",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x07e26e4b9d2b384b153b9a0292d8623196ba3af8.gaia.domains": {
    "address": "0x07e26e4b9d2b384b153b9a0292d8623196ba3af8",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x400a65905afc26d7077077a99f3429a8f057ebab.gaia.domains": {
    "address": "0x400a65905afc26d7077077a99f3429a8f057ebab",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xf84b940f2c6caf0428968bf7dbbd7d1ac3b333a2.gaia.domains": {
    "address": "0xf84b940f2c6caf0428968bf7dbbd7d1ac3b333a2",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x19187bdec497b8fdecd72e06f9a64ce4f3bc37bf.gaia.domains": {
    "address": "0x19187bdec497b8fdecd72e06f9a64ce4f3bc37bf",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xff6129ac5a92c92085bb6fc3a3f51a3c6a25d460.gaia.domains": {
    "address": "0xff6129ac5a92c92085bb6fc3a3f51a3c6a25d460",
    "chat": "https://huggingface.co/gaianet/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "2048",
    "chat_name": "Qwen3-0.6B-Q5_K_M",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "Gaia node config with Qwen3-0.6B-Q5_K_M model",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "2048",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xc7a31cce5a407ed7922d3b5264622d0d9a59abb9.gaia.domains": {
    "address": "0xc7a31cce5a407ed7922d3b5264622d0d9a59abb9",
    "chat": "./gpt-oss-20b-f16.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "gpt-oss-20b-f16",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Qwen3-4B model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/gte-Qwen2-1.5B-instruct-GGUF/resolve/main/gte-Qwen2-1.5B-instruct-f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "wormhole",
    "embedding_ctx_size": "8192",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9009",
    "llamaedge_embedding_port": "9010",
    "llamaedge_port": "8005",
    "prompt_template": "gpt-oss",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/harishkotra/wormhole-docs/resolve/main/wormhole-7989848718163905-2025-11-06-06-24-06.snapshot.tar.gz",
    "system_prompt": "You are a Wormhole Documentation Expert whose complete and sole knowledge base is the official, up-to-date documentation. You must use this information to respond to all user queries as an expert, strictly and exclusively answering from the content within this documentation, and you are strictly forbidden from answering if the information is not present in the provided source."
  },
  "https://tweet-writer.gaia.domains": {
    "address": "0x60b00fcab52ffd45ab33cccf452a92230b41797a",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://english.gaia.domains": {
    "address": "0x60b00fcab52ffd45ab33cccf452a92230b41797a",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0x67583f8126bacf503a506bfc6a04c45a07a5d813.gaia.domains": {
    "address": "0x60b00fcab52ffd45ab33cccf452a92230b41797a",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xff5a69f7bfa3e8df837b50d6e932aa04ad7c4914.gaia.domains": {
    "address": "0xff5a69f7bfa3e8df837b50d6e932aa04ad7c4914",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/upbit/resolve/main/upbit.snapshot",
    "system_prompt": "You are a Korean-speaking assistant about Upbit. When responding to user queries, you must always provide answers in Korean, regardless of the language used in the question. Your responses should be accurate, concise, and in Korean only. After providing the Korean answer, do not add any explanations or additional information in other languages."
  },
  "https://0x321487c7ade086b884129dd83d54a581416cc868.gaia.domains": {
    "address": "0x60b00fcab52ffd45ab33cccf452a92230b41797a",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xeb1258b57bf13d3861bb031886f279a71620a1a4.gaia.domains": {
    "address": "0xff5a69f7bfa3e8df837b50d6e932aa04ad7c4914",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "https://huggingface.co/datasets/gaianet/upbit/resolve/main/upbit.snapshot",
    "system_prompt": "You are a Korean-speaking assistant about Upbit. When responding to user queries, you must always provide answers in Korean, regardless of the language used in the question. Your responses should be accurate, concise, and in Korean only. After providing the Korean answer, do not add any explanations or additional information in other languages."
  },
  "https://0x652a9619ac5a4aad79b404e069574b4c2a22ed0b.gaia.domains": {
    "address": "0x652a9619ac5a4aad79b404e069574b4c2a22ed0b",
    "chat": "https://huggingface.co/second-state/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Qwen3-4B",
    "chat_ubatch_size": "128",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "qwen3-no-think",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  },
  "https://0xec2fb21621c021d3d7887858acea0ff7fd9699b2.gaia.domains": {
    "address": "0xec2fb21621c021d3d7887858acea0ff7fd9699b2",
    "chat": "https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
    "chat_batch_size": "128",
    "chat_ctx_size": "16384",
    "chat_name": "Llama-3.2-3B-Instruct",
    "chat_ubatch_size": "128",
    "context_window": "1",
    "description": "The default GaiaNet node config with a Llama-3.2-3B-Instruct model.",
    "domain": "gaia.domains",
    "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
    "embedding_batch_size": "8192",
    "embedding_collection_name": "default",
    "embedding_ctx_size": "8192",
    "embedding_name": "Nomic-embed-text-v1.5",
    "embedding_ubatch_size": "8192",
    "llamaedge_chat_port": "9068",
    "llamaedge_embedding_port": "9069",
    "llamaedge_port": "8080",
    "prompt_template": "llama-3-chat",
    "qdrant_limit": "1",
    "qdrant_score_threshold": "0.5",
    "rag_policy": "system-message",
    "rag_prompt": "Use the following information to answer the question.\n----------------\n",
    "reverse_prompt": "",
    "snapshot": "",
    "system_prompt": "You are a helpful AI assistant. Please answer questions as clearly and concisely as possible."
  }
}